{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836a85c6",
   "metadata": {},
   "source": [
    "**Objetivo de esta etapa:** Entrenar múltiples versiones del modelo MF con diferentes combinaciones de hiperparámetros y comparar su rendimiento\n",
    "\n",
    "**Grid Search:** Prueba todas las combinaciones posibles de un conjunto de valores. Es exhaustivo pero lento.\n",
    "\n",
    "**Random Search**: Elige combinaciones al azar durante X intentos. Mucho más rápido, y en la práctica muy efectivo.\n",
    "\n",
    "**Algoritmos genéticos/Optuna:** Estrategias inteligentes que aprenden qué combinaciones probar. Pueden encontrar buenas configuraciones en menos tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d107410",
   "metadata": {},
   "source": [
    "\n",
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0385c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornac.data import Reader\n",
    "from cornac.eval_methods import RatioSplit\n",
    "\n",
    "\n",
    "# Load MovieLens 100K dataset\n",
    "reader = Reader()\n",
    "ml_data = reader.read(fpath='./datasets/ml-100k/u.data', fmt='UIRT', sep='\\t')\n",
    "\n",
    "# Define the evaluation metrics\n",
    "eval_method = RatioSplit(\n",
    "    data=ml_data,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=0.0,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product   # generar todas las combinaciones posibles de varios conjuntos de valores\n",
    "from cornac.models import MF\n",
    "from cornac.metrics import RMSE, MAE, Precision, NDCG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Hiperparámetros que vamos a probar\n",
    "k_list = [10, 20]                 # Dimensión de los vectores latentes\n",
    "lr_list = [0.01, 0.005]           # Velocidad de aprendizaje\n",
    "reg_list = [0.01, 0.1]            # Regularización L2 (evita sobreajuste)\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for k, lr, reg in product(k_list, lr_list, reg_list):\n",
    "    print(f\"\\nEntrenando MF con k={k}, lr={lr}, reg={reg}\")\n",
    "    \n",
    "    model = MF(\n",
    "        k=k,\n",
    "        learning_rate=lr,\n",
    "        lambda_reg=reg,\n",
    "        max_iter=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    model.fit(eval_method.train_set)\n",
    "\n",
    "    metrics = eval_method.evaluate(\n",
    "        model,\n",
    "        metrics=[RMSE(), MAE(), Precision(k=5), NDCG(k=5)],\n",
    "        user_based=True\n",
    "    )\n",
    "    \n",
    "    res = dict(metrics[0].metric_avg_results)\n",
    "    res.update({\"k\": k, \"lr\": lr, \"reg\": reg})\n",
    "    resultados.append(res)\n",
    "\n",
    "# Convertir resultados a DataFrame y ordenar por RMSE\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados = df_resultados.sort_values(by=\"RMSE\")\n",
    "\n",
    "# Mostrar tabla ordenada por RMSE\n",
    "print(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2e93d",
   "metadata": {},
   "source": [
    "**Tabla de resultados de Grid Search de hiperparámetros sobre MF**\n",
    "\n",
    "| k  | lr    | reg  | RMSE       | MAE        | Precision\\@5 | NDCG\\@5    | Train (s) | Test (s) |\n",
    "| -- | ----- | ---- | ---------- | ---------- | ------------ | ---------- | --------- | -------- |\n",
    "| 10 | 0.005 | 0.01 | 0.9376     | 0.7621     | **0.0881**   | **0.0902** | 0.021     | 0.711    |\n",
    "| 20 | 0.005 | 0.01 | 0.9610     | 0.7800     | 0.0809       | 0.0839     | 0.083     | 0.997    |\n",
    "| 10 | 0.010 | 0.01 | 0.9831     | 0.7895     | 0.0415       | 0.0407     | 0.055     | 1.241    |\n",
    "| 20 | 0.010 | 0.01 | 1.0411     | 0.8415     | 0.0500       | 0.0502     | 0.021     | 0.681    |\n",
    "| 20 | 0.005 | 0.10 | **0.8894** | **0.7317** | 0.0347       | 0.0297     | 0.047     | 0.675    |\n",
    "| 10 | 0.005 | 0.10 | 0.8912     | 0.7334     | 0.0302       | 0.0251     | 0.065     | 0.652    |\n",
    "| 10 | 0.010 | 0.10 | 0.8913     | 0.7331     | 0.0187       | 0.0166     | 0.044     | 0.644    |\n",
    "| 20 | 0.010 | 0.10 | 0.8924     | 0.7332     | 0.0228       | 0.0203     | 0.035     | 0.642    |\n",
    "\n",
    "**Mejor precisión numérica (RMSE más bajo)**\n",
    "\n",
    "* `k=20`, `lr=0.005`, `reg=0.1`\n",
    "* Mejor RMSE\n",
    "* Pero baja Precision\\@5 y NDCG\\@5 (solo 3.4%)\n",
    "\n",
    "Este modelo predice bien los ratings, pero no rankea bien los ítems (el orden no coincide con lo que el usuario desea).\n",
    "\n",
    "\n",
    "**Mejor ranking (Precision\\@5 y NDCG\\@5)**\n",
    "\n",
    "  * `k=10`, `lr=0.005`, `reg=0.01`\n",
    "  * Precision\\@5 = 0.0881\n",
    "  * NDCG\\@5 = 0.0902\n",
    "  * RMSE = 0.9376 (más alto que el mínimo, pero aceptable)\n",
    "\n",
    "Este modelo recomienda mejor el Top-N, aunque no predice los ratings con tanta precisión.\n",
    "\n",
    "**Conclusiones**\n",
    "\n",
    "1. No hay una sola combinación “mejor”. Depende del objetivo:\n",
    "\n",
    "   * ¿Predecir bien el rating? → RMSE bajo.\n",
    "   * ¿Que el Top-5 recomendado sea útil? → Precision\\@5 alto.\n",
    "2. Regularización alta (reg=0.1) mejora RMSE pero perjudica Precision\\@5.\n",
    "3. Más dimensiones (k=20) mejora RMSE pero no siempre mejora ranking.\n",
    "4. La tasa de aprendizaje (lr) más baja (0.005) da mejores resultados en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67061e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear etiquetas para el eje X: combinación de hiperparámetros como string\n",
    "df_resultados['config'] = df_resultados.apply(\n",
    "    lambda row: f\"k={row['k']},lr={row['lr']},reg={row['reg']}\", axis=1\n",
    ")\n",
    "\n",
    "# Ordenar por RMSE para una visualización clara\n",
    "df_resultados = df_resultados.sort_values(by='RMSE')\n",
    "\n",
    "# Crear gráfico de doble eje Y\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# RMSE en eje Y izquierdo\n",
    "ax1.plot(df_resultados['config'], df_resultados['RMSE'], 'o-', label='RMSE', color='tab:red')\n",
    "ax1.set_ylabel('RMSE', color='tab:red')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "# Precision@5 en eje Y derecho\n",
    "ax2.plot(df_resultados['config'], df_resultados['Precision@5'], 's--', label='Precision@5', color='tab:blue')\n",
    "ax2.set_ylabel('Precision@5', color='tab:blue')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Estética general\n",
    "plt.title(\"Comparación de RMSE y Precision@5 según hiperparámetros\")\n",
    "ax1.set_xticks(range(len(df_resultados)))\n",
    "ax1.set_xticklabels(df_resultados['config'], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cornac-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
