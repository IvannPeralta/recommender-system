{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías\n",
    "import cornac\n",
    "from cornac.data import Reader\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import MF, PMF\n",
    "from cornac.metrics import RMSE, MAE, Precision, NDCG\n",
    "\n",
    "# Cargar los datos\n",
    "reader = Reader()\n",
    "ml_data = reader.read(fpath='datasets/ml-100k/u.data', fmt='UIRT', sep='\\t')\n",
    "\n",
    "# Crear método de evaluación (split)\n",
    "eval_method = RatioSplit(\n",
    "    data=ml_data,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=0.0,  # considera todos los ratings\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f5d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: Matrix Factorization (MF)\n",
    "model_mf = MF(k=10, max_iter=50, learning_rate=0.01, verbose=True)\n",
    "\n",
    "mf_results = eval_method.evaluate(model_mf, metrics=[RMSE(), MAE(), Precision(k=5), NDCG(k=5)], user_based=True)\n",
    "\n",
    "for metric, value in mf_results[0].metric_avg_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: PMF\n",
    "model_pmf = PMF(k=10, max_iter=50, learning_rate=0.01, verbose=True)\n",
    "\n",
    "pmf_results = eval_method.evaluate(model_pmf, metrics=[RMSE(), MAE(), Precision(k=5), NDCG(k=5)], user_based=True)\n",
    "\n",
    "for metric, value in pmf_results[0].metric_avg_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77a6fd",
   "metadata": {},
   "source": [
    "| Modelo | RMSE   | MAE    | Precision@5  | NDCG@5  | TRAIN(S) | TEST(S)  |\n",
    "|--------|--------|--------|--------------|---------|----------|----------|\n",
    "| MF     | 0.9239 | 0.7507 | 0.0732       | 0.0762  | 0.1583   | 0.7775   |\n",
    "| PMF    | 1.0140 | 0.8266 | 0.0506       | 0.0492  | 0.7237   | 0.5035   |\n",
    "\n",
    "\n",
    "\n",
    "#### **Qué mide cada métrica?**\n",
    "\n",
    "| Métrica          | Qué mide exactamente                                                                                | Valor ideal                                 |\n",
    "| ---------------- | --------------------------------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **RMSE**         | *Root Mean Squared Error*: error promedio cuadrático. Penaliza fuerte los errores grandes.          | Lo más bajo posible                         |\n",
    "| **MAE**          | *Mean Absolute Error*: promedio del error absoluto entre rating real y predicho.                    | Lo más bajo posible                         |\n",
    "| **Precision\\@5** | De los 5 ítems recomendados a cada usuario, ¿qué proporción fueron relevantes?                      | Más alto = mejor                            |\n",
    "| **NDCG\\@5**      | *Normalized Discounted Cumulative Gain*: similar a Precision, pero considera el orden de los ítems. | Más alto = mejor                            |\n",
    "| **TRAIN(S)**     | Tiempo de entrenamiento del modelo, en segundos.                                                    | Más bajo es mejor (si hay mucha diferencia) |\n",
    "| **TEST(S)**      | Tiempo de evaluación del modelo (predicción y métricas).                                            | Más bajo es mejor (si afecta escalabilidad) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpretación por métrica**\n",
    "\n",
    "**RMSE y MAE:**\n",
    "\n",
    "* MF tiene menor error (0.92 vs 1.01 y 0.75 vs 0.82).\n",
    "* Esto significa que MF predice mejor los ratings reales, con menos desviación promedio que PMF.\n",
    "\n",
    "**Precision\\@5 y NDCG\\@5:**\n",
    "\n",
    "* MF también supera claramente a PMF:\n",
    "\n",
    "  * Precision\\@5: MF logra \\~7.3% de acierto, mientras PMF solo \\~5.0%.\n",
    "  * NDCG\\@5: MF \\~7.6%, PMF \\~4.9%.\n",
    "* Esto indica que MF recomienda ítems más relevantes y en mejor orden que PMF.\n",
    "\n",
    "> Aunque parezcan valores bajos, es normal en datasets grandes y dispersos como MovieLens 100k.\n",
    "\n",
    "##### **Conclusiones generales**\n",
    "\n",
    "1. MF es un modelo más preciso y eficiente que PMF en este dataset.\n",
    "2. PMF puede ser útil en escenarios probabilísticos, pero en este caso no ofrece ventajas claras.\n",
    "3. Precision\\@k y NDCG son métricas clave cuando lo que te importa no es el número exacto del rating, sino si recomendás lo que el usuario va a disfrutar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cornac-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
