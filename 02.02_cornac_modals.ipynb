{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bd7b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 0.0\n",
      "exclude_unknowns = True\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1651\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 943\n",
      "Number of items = 1651\n",
      "Number of ratings = 19964\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 0\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1651\n"
     ]
    }
   ],
   "source": [
    "# Importar librerías\n",
    "import cornac\n",
    "from cornac.data import Reader\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import MF, PMF\n",
    "from cornac.metrics import RMSE, MAE, Precision, NDCG\n",
    "\n",
    "# Cargar los datos\n",
    "reader = Reader()\n",
    "ml_data = reader.read(fpath='datasets/ml-100k/u.data', fmt='UIRT', sep='\\t')\n",
    "\n",
    "# Crear método de evaluación (split)\n",
    "eval_method = RatioSplit(\n",
    "    data=ml_data,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=0.0,  # considera todos los ratings\n",
    "    seed=42,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95f5d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MF] Training started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725e2a556a2943cb89a8a5ec7dd9640f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization finished!\n",
      "\n",
      "[MF] Evaluation started!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa30c78947f43978f71f18c8836107d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Rating:   0%|          | 0/19964 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcee44ad7a634704b32873bfa9d32861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ranking:   0%|          | 0/940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7475\n",
      "RMSE: 0.9185\n",
      "NDCG@5: 0.0798\n",
      "Precision@5: 0.0789\n",
      "Train (s): 0.3127\n",
      "Test (s): 1.6402\n"
     ]
    }
   ],
   "source": [
    "# Modelo 1: Matrix Factorization (MF)\n",
    "model_mf = MF(k=10, max_iter=50, learning_rate=0.01, verbose=True)\n",
    "\n",
    "mf_results = eval_method.evaluate(model_mf, metrics=[RMSE(), MAE(), Precision(k=5), NDCG(k=5)], user_based=True)\n",
    "\n",
    "for metric, value in mf_results[0].metric_avg_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: Probabilistic Matrix Factorization (PMF) \n",
    "model_pmf = PMF(k=10, max_iter=50, learning_rate=0.01, verbose=True)\n",
    "\n",
    "pmf_results = eval_method.evaluate(model_pmf, metrics=[RMSE(), MAE(), Precision(k=5), NDCG(k=5)], user_based=True)\n",
    "\n",
    "for metric, value in pmf_results[0].metric_avg_results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d77a6fd",
   "metadata": {},
   "source": [
    "| Modelo | RMSE   | MAE    | Precision@5  | NDCG@5  | TRAIN(S) | TEST(S)  |\n",
    "|--------|--------|--------|--------------|---------|----------|----------|\n",
    "| MF     | 0.9239 | 0.7507 | 0.0732       | 0.0762  | 0.1583   | 0.7775   |\n",
    "| PMF    | 1.0140 | 0.8266 | 0.0506       | 0.0492  | 0.7237   | 0.5035   |\n",
    "\n",
    "\n",
    "\n",
    "#### **Qué mide cada métrica?**\n",
    "\n",
    "| Métrica          | Qué mide exactamente                                                                                | Valor ideal                                 |\n",
    "| ---------------- | --------------------------------------------------------------------------------------------------- | ------------------------------------------- |\n",
    "| **RMSE**         | *Root Mean Squared Error*: error promedio cuadrático. Penaliza fuerte los errores grandes.          | Lo más bajo posible                         |\n",
    "| **MAE**          | *Mean Absolute Error*: promedio del error absoluto entre rating real y predicho.                    | Lo más bajo posible                         |\n",
    "| **Precision\\@5** | De los 5 ítems recomendados a cada usuario, ¿qué proporción fueron relevantes?                      | Más alto = mejor                            |\n",
    "| **NDCG\\@5**      | NDCG@k (Normalized Discounted Cumulative Gain) mide no solo cuántos ítems relevantes hay en el Top-k, sino también su posición. | Más alto = mejor                            |\n",
    "| **TRAIN(S)**     | Tiempo de entrenamiento del modelo, en segundos.                                                    | Más bajo es mejor (si hay mucha diferencia) |\n",
    "| **TEST(S)**      | Tiempo de evaluación del modelo (predicción y métricas).                                            | Más bajo es mejor (si afecta escalabilidad) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Interpretación por métrica**\n",
    "\n",
    "**RMSE y MAE:**\n",
    "\n",
    "* MF tiene menor error (0.92 vs 1.01 y 0.75 vs 0.82).\n",
    "* Esto significa que MF predice mejor los ratings reales, con menos desviación promedio que PMF.\n",
    "\n",
    "**Precision\\@5 y NDCG\\@5:**\n",
    "\n",
    "* MF también supera claramente a PMF:\n",
    "\n",
    "  * Precision\\@5: MF logra \\~7.3% de acierto, mientras PMF solo \\~5.0%.\n",
    "  * NDCG\\@5: MF \\~7.6%, PMF \\~4.9%.\n",
    "* Esto indica que MF recomienda ítems más relevantes y en mejor orden que PMF.\n",
    "\n",
    "> Aunque parezcan valores bajos, es normal en datasets grandes y dispersos como MovieLens 100k.\n",
    "\n",
    "##### **Conclusiones generales**\n",
    "\n",
    "1. MF es un modelo más preciso y eficiente que PMF en este dataset.\n",
    "2. PMF puede ser útil en escenarios probabilísticos, pero en este caso no ofrece ventajas claras.\n",
    "3. Precision\\@k y NDCG son métricas clave cuando lo que te importa no es el número exacto del rating, sino si recomendás lo que el usuario va a disfrutar.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cornac-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
